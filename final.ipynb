{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py\n",
    "from collections import Counter\n",
    "import nltk\n",
    "#nltk.download('punkt') if needed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Merge\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import base_filter\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Embedding\n",
    "import scipy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_question_file = ''\n",
    "train_answer_file = ''\n",
    "val_question_file = ''\n",
    "val_answer_file = ''\n",
    "\n",
    "# column for question -> 'questions'\n",
    "#            val      -> 'annotations'\n",
    "\n",
    "def json_pandas(json_file,column):\n",
    "    with open(json_file) as f:\n",
    "        json = json.loads(f.read())\n",
    "\n",
    "    df = pd.DataFrame(json[column])\n",
    "    del json\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_question = json_pandas(train_question_file,'questions')\n",
    "train_answer = json_pandas(train_answer_file,'annotations')\n",
    "test_question = json_pandas(val_question_file,'questions')\n",
    "test_answer = json_pandas(val_answer_file,'annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_raw_data(ques_df,ans_df):\n",
    "    ques = ques_df.ix[:,[0,2,3]]\n",
    "    ques['answer']=ans_df.ix[:,3]\n",
    "    df = ques.drop(['multiple_choices','question_id'],axis=1)\n",
    "    del ques_df,ans_df,ques\n",
    "    return df\n",
    "\n",
    "train = clean_raw_data(train_question,train_answer)\n",
    "test = clean_raw_data(train_question,train_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make pickle or h5py\n",
    "#train.to_pickle(file_name)\n",
    "#test.to_pickle(file_name)\n",
    "#or\n",
    "#train.to_hdf(file_name)\n",
    "#test.to_hdf(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_word = 1000\n",
    "max_seq = 20\n",
    "\n",
    "def dataframe_list(df):\n",
    "    ques_list = [s.encode('ascii') for s in list(df.question.values)]\n",
    "    ans_list = [s.encode('ascii') for s in list(df.answer.values)]\n",
    "    image_list = df.image_id.values.tolist()\n",
    "    return ques_list,ans_list,image_list\n",
    "\n",
    "train_q,train_a,train_img = dataframe_list(train)\n",
    "test_q,test_a,test_img = dataframe_list(test)\n",
    "\n",
    "def tokeniz(txt,mode='default',max_word_size=None):\n",
    "    \n",
    "    if mode is 'default':\n",
    "        tokenizer = Tokenizer(nb_words=None, filters=base_filter(), lower=True, split=\" \")\n",
    "        tokenizer.fit_on_texts(txt)\n",
    "        sequences = tokenizer.texts_to_sequences(txt)\n",
    "        \n",
    "    elif mode is 'question':\n",
    "        tokenizer = Tokenizer(nb_words=max_word_size, filters=base_filter(), lower=True, split=\" \")\n",
    "        tokenizer.fit_on_texts(txt)\n",
    "        sequences = tokenizer.texts_to_sequences(txt)\n",
    "        word_index = tokenizer.word_index\n",
    "        data = pad_sequences(sequences, maxlen=max_seq)\n",
    "        return data\n",
    "    \n",
    "    elif mode is 'answer':\n",
    "        tokenizer = Tokenizer(nb_words=None, filters=base_filter(), lower=True, split=\" \")\n",
    "        tokenizer.fit_on_texts(txt)\n",
    "        sequences = tokenizer.texts_to_sequences(txt)\n",
    "        word_index = tokenizer.word_index\n",
    "        sort_freq = sorted(word_index.items(),key=operator.itemgetter(1),reverse=True)[0:max_word]\n",
    "        top_answers, top_fq = zip(*sort_freq)\n",
    "        labels = np_utils.to_categorical(np.asarray(top_fq))\n",
    "        data = np.zeros(shape(len(txt),max_word))\n",
    "        for i in range(len(txt)):\n",
    "            data[i]=labels[top_answer.index(txt[i])]\n",
    "        return labels\n",
    "\n",
    "X_train = tokeniz(train_q,'question')\n",
    "Y_train = tokeniz(train_a,'answer')\n",
    "X_test = tokeniz(test_q,'question')\n",
    "Y_test = tokeniz(test_a,'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_labels(pickle_file_path):\n",
    "    data_frame = pd.read_pickle(pickle_file_path)\n",
    "    labels = data_frame[['image_id']].values\n",
    "    return labels\n",
    "\n",
    "def get_image_features(img_ids,vgg_model_path):\n",
    "    features_struct = sc.io.loadmat(vgg_model_path)\n",
    "    VGGfeatures = features_struct['feats']\n",
    "    id_map = {}\n",
    "    for ids in img_ids:\n",
    "        ids_split = ids.split()\n",
    "        id_map[id_split[0]] = int(id_split[1])\n",
    "    nb_samples = len(img_ids)\n",
    "    nb_dimensions = VGGfeatures.shape[0]\n",
    "    image_matrix = np.zeros((nb_samples, nb_dimensions))\n",
    "    for j in range(nb_samples):\n",
    "        image_matrix[j,:] = VGGfeatures[:,id_map[img_ids[j]]]\n",
    "    return image_matrix\n",
    "\n",
    "X_train_img = get_image_features(train_img)\n",
    "X_test_img = get_image_features(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "\n",
    "with open('embeddings/embedding_matrix','r') as f:\n",
    "    embedding = pickle.load(file)\n",
    "with open('embeddings/word_idx','r') as f:\n",
    "    word_idx = pickle.load(file)\n",
    "\n",
    "embedding_matrix = np.zeros(shape=(len(word_index)+1), embed_dim)\n",
    "for word,freq in word_index.items():\n",
    "    embedding_matrix[freq] = embedding[word_idx[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_vgg = Sequential()\n",
    "left_vgg.add(Dense(300, input_dim=4096, activation='relu'))\n",
    "\n",
    "centre_w2vec = Sequential()\n",
    "embedding_layer = Embedding(len(word_index) + 1,embed_dim,weights=[embedding_matrix],input_length=max_seq,trainable=False)\n",
    "#centre_w2vec.add(Dense(500,input_dim=300))\n",
    "\n",
    "right_vgg = Sequential()\n",
    "right_vgg.add(Dense(300, input_dim=4096, activation='relu'))\n",
    "\n",
    "merge_layer = Merge([left_vgg,centre_w2vec,right_vgg], mode='concat')\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(merge_layer)\n",
    "lstm_model.add(Dropout(dropout_rate))\n",
    "lstm_model.add(LSTM(1000, input_shape=(1+max_seq+1,300)))\n",
    "lstm_model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model.fit([X_train_img, X_train, X_train_img], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
