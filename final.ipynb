{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py\n",
    "from collections import Counter\n",
    "import nltk\n",
    "#nltk.download('punkt') if needed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Flatten, Embedding, Merge\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import base_filter\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Embedding\n",
    "import scipy as sc\n",
    "from keras.layers.core import Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_question_file = 'data/Q_T.json'\n",
    "train_answer_file = 'data/A_T.json'\n",
    "val_question_file = 'data/Q_V.json'\n",
    "val_answer_file = 'data/A_V.json'\n",
    "\n",
    "# column for question -> 'questions'\n",
    "#            val      -> 'annotations'\n",
    "\n",
    "def json_pandas(json_file,column):\n",
    "    with open(json_file) as f:\n",
    "        json_file = json.loads(f.read())\n",
    "\n",
    "    df = pd.DataFrame(json_file[column])\n",
    "    del json_file\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_question = json_pandas(train_question_file,'questions')\n",
    "train_answer = json_pandas(train_answer_file,'annotations')\n",
    "test_question = json_pandas(val_question_file,'questions')\n",
    "test_answer = json_pandas(val_answer_file,'annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_raw_data(ques_df,ans_df):\n",
    "    ques = ques_df.ix[:,[0,2,3]]\n",
    "    ques['answer']=ans_df.ix[:,3]\n",
    "    df = ques.drop(['question_id'],axis=1)\n",
    "    del ques_df,ans_df,ques\n",
    "    return df\n",
    "\n",
    "train = clean_raw_data(train_question,train_answer)\n",
    "test = clean_raw_data(test_question,test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[['question']] = train[['question']].astype(str) \n",
    "train[['answer']] = train[['answer']].astype(str) \n",
    "test[['question']] = test[['question']].astype(str) \n",
    "test[['answer']] = test[['answer']].astype(str) \n",
    "hdf = pd.HDFStore('data.h5')\n",
    "hdf.put('train',train,format='table',data_columns=True)\n",
    "hdf.put('test',test,format='table',data_columns=True)\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_hdf('data.h5','test')\n",
    "train = pd.read_hdf('data.h5','train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataframe_list(df):\n",
    "#     ques_list = [s.encode('ascii') for s in list(df.question.values)]\n",
    "#     ans_list = [s.encode('ascii') for s in list(df.answer.values)]\n",
    "    ques_list = df.question.values.tolist()\n",
    "    ans_list = df.answer.values.tolist()\n",
    "    image_list = df.image_id.values.tolist()\n",
    "    return ques_list,ans_list,image_list\n",
    "\n",
    "train_q,train_a,train_img = dataframe_list(train)\n",
    "#test_q,test_a,test_img = dataframe_list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_image_features(img_ids,vgg_model_path):\n",
    "    features_struct = sc.io.loadmat(vgg_model_path)\n",
    "    VGGfeatures = features_struct['feats']\n",
    "    id_map = {}\n",
    "    for ids in img_ids:\n",
    "        ids_split = ids.split()\n",
    "        id_map[id_split[0]] = int(id_split[1])\n",
    "    nb_samples = len(img_ids)\n",
    "    nb_dimensions = VGGfeatures.shape[0]\n",
    "    image_matrix = np.zeros((nb_samples, nb_dimensions))\n",
    "    for j in range(nb_samples):\n",
    "        image_matrix[j,:] = VGGfeatures[:,id_map[img_ids[j]]]\n",
    "    return image_matrix\n",
    "\n",
    "get_image_features(train_img,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dic_question(ques_list,file_path='glove.840B.300d.txt',ques_size=25):\n",
    "    embedding_index = {}\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = coefs\n",
    "    embedding_martix = embedding_index.values()\n",
    "    hdf = h5py.File('embedded_matrix.h5', 'w')\n",
    "    hdf.create_dataset('matrix', data=embedding_martix)\n",
    "    hdf.close()\n",
    "    dic = {}\n",
    "    embedding_ids = embedding_index.keys()\n",
    "    for ids,words in enumerate(embedding_ids):\n",
    "        dic[words] = ids\n",
    "    \n",
    "    return dic\n",
    "\n",
    "dic = create_dic_question(train_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('emb_dic.pickle', 'wb') as f:\n",
    "    pickle.dump(dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('emb_dic.pickle', 'rb') as f:\n",
    "    dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_ans_dic(train_a,max_val=1000):\n",
    "    import operator\n",
    "    from collections import defaultdict\n",
    "    freq = defaultdict(int)\n",
    "    for line in train_a:\n",
    "        for word in nltk.word_tokenize(str(line)):\n",
    "            freq[word.lower()]+=1\n",
    "\n",
    "    sort_freq = sorted(freq.items(),key=operator.itemgetter(1),reverse=True)[0:max_val]\n",
    "    top_answers, top_fq = zip(*sort_freq)\n",
    "    dic = {}\n",
    "    for ids,words in enumerate(top_answers):\n",
    "        dic[words] = ids\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ques_matrix(ques_list,dic,ques_size=25):\n",
    "    \n",
    "    ques_matrix = np.zeros(shape=(len(ques_list),ques_size))\n",
    "    \n",
    "    for i in range(len(ques_list)):\n",
    "        words = nltk.word_tokenize(ques_list[i])\n",
    "        for j in range(len(words)):\n",
    "            try :\n",
    "                ques_matrix[i,-len(words)+j] = dic[words[j]]\n",
    "            except:\n",
    "                ques_matrix[i,-len(words)+j] = 0\n",
    "    \n",
    "    return ques_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = get_ques_matrix(train_q,dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans_dic = create_ans_dic(train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_ans_matrix(ans_list,dic,max_value=1001):    \n",
    "    ans_matrix = np.zeros(shape=(len(ans_list),max_value))\n",
    "    \n",
    "    for i in range(len(ans_list[:10])):\n",
    "#         print ans_list[i]\n",
    "#         print dic[ans_list[i]]\n",
    "        try:\n",
    "            ans_matrix[i,dic[ans_list[i]]]=1\n",
    "        except:\n",
    "            ans_matrix[i,1000]=1\n",
    "    return ans_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = get_ans_matrix(train_a,ans_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_word = 1000\n",
    "max_seq = 20\n",
    "\n",
    "def dataframe_list(df):\n",
    "    ques_list = [s.encode('ascii') for s in list(df.question.values)]\n",
    "    ans_list = [s.encode('ascii') for s in list(df.answer.values)]\n",
    "    image_list = df.image_id.values.tolist()\n",
    "    return ques_list,ans_list,image_list\n",
    "\n",
    "train_q,train_a,train_img = dataframe_list(train)\n",
    "test_q,test_a,test_img = dataframe_list(test)\n",
    "\n",
    "def tokeniz(txt,mode='default',max_word_size=None):\n",
    "    \n",
    "    if mode is 'default':\n",
    "        tokenizer = Tokenizer(nb_words=None, filters=base_filter(), lower=True, split=\" \")\n",
    "        tokenizer.fit_on_texts(txt)\n",
    "        sequences = tokenizer.texts_to_sequences(txt)\n",
    "        \n",
    "    elif mode is 'question':\n",
    "        tokenizer = Tokenizer(nb_words=max_word_size, filters=base_filter(), lower=True, split=\" \")\n",
    "        tokenizer.fit_on_texts(txt)\n",
    "        sequences = tokenizer.texts_to_sequences(txt)\n",
    "        word_index = tokenizer.word_index\n",
    "        data = pad_sequences(sequences, maxlen=max_seq)\n",
    "        return data\n",
    "    \n",
    "    elif mode is 'answer':\n",
    "        tokenizer = Tokenizer(nb_words=None, filters=base_filter(), lower=True, split=\" \")\n",
    "        tokenizer.fit_on_texts(txt)\n",
    "        sequences = tokenizer.texts_to_sequences(txt)\n",
    "        word_index = tokenizer.word_index\n",
    "        sort_freq = sorted(word_index.items(),key=operator.itemgetter(1),reverse=True)[0:max_word]\n",
    "        top_answers, top_fq = zip(*sort_freq)\n",
    "        labels = np_utils.to_categorical(np.asarray(top_fq))\n",
    "        data = np.zeros(shape(len(txt),max_word))\n",
    "        for i in range(len(txt)):\n",
    "            data[i]=labels[top_answer.index(txt[i])]\n",
    "        return labels\n",
    "\n",
    "X_train = tokeniz(train_q,'question')\n",
    "Y_train = tokeniz(train_a,'answer')\n",
    "X_test = tokeniz(test_q,'question')\n",
    "Y_test = tokeniz(test_a,'answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_labels(pickle_file_path):\n",
    "    data_frame = pd.read_pickle(pickle_file_path)\n",
    "    labels = data_frame[['image_id']].values\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "X_train_img = get_image_features(train_img)\n",
    "X_test_img = get_image_features(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "\n",
    "with open('embeddings/embedding_matrix','r') as f:\n",
    "    embedding = pickle.load(file)\n",
    "with open('embeddings/word_idx','r') as f:\n",
    "    word_idx = pickle.load(file)\n",
    "\n",
    "embedding_matrix = np.zeros(shape=(len(word_index)+1), embed_dim)\n",
    "for word,freq in word_index.items():\n",
    "    embedding_matrix[freq] = embedding[word_idx[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left_vgg = Sequential()\n",
    "left_vgg.add(Dense(300, input_dim=4096, activation='relu'))\n",
    "\n",
    "centre_w2vec = Sequential()\n",
    "embedding_layer = Embedding(len(word_index) + 1,embed_dim,weights=[embedding_matrix],input_length=max_seq,trainable=False)\n",
    "#centre_w2vec.add(Dense(500,input_dim=300))\n",
    "\n",
    "right_vgg = Sequential()\n",
    "right_vgg.add(Dense(300, input_dim=4096, activation='relu'))\n",
    "\n",
    "merge_layer = Merge([left_vgg,centre_w2vec,right_vgg], mode='concat')\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(merge_layer)\n",
    "lstm_model.add(Dropout(dropout_rate))\n",
    "lstm_model.add(LSTM(1000, input_shape=(1+max_seq+1,300)))\n",
    "lstm_model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model.fit([X_train_img, X_train, X_train_img], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros(shape=(400,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_network():\n",
    "#         embedding_matrix = embedding.load()\n",
    "        embedding_model = Sequential()\n",
    "        embedding_model.add(Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],weights = [embedding_matrix],input_length = 25,trainable = False ))\n",
    "        image_model = Sequential()\n",
    "        image_model.add(Dense(embedding_matrix.shape[1],input_dim=4096,activation='linear' ))\n",
    "        image_model.add(Reshape((1,embedding_matrix.shape[1])))\n",
    "        main_model = Sequential()\n",
    "        main_model.add(Merge([image_model,embedding_model],mode = 'concat',concat_axis = 1))\n",
    "        main_model.add(LSTM(1001))\n",
    "        main_model.add(Dense(1001,activation='softmax'))\n",
    "        return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(model, to_file='model1.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ROUGH WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tron/Desktop/VQA\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_T.json  A_V.json  Q_T.json  Q_V.json\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
